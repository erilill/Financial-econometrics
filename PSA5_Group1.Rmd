---
title: "Report for PS5"
author:
- Mark Becker, Erik Lillrank & Vilim Nedic
date: ''
output:
  pdf_document:
    keep_tex: true
  df_print: kable
  html_document:
    df_print: paged
  word_document: default
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{multirow}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{adjustbox}
  - \usepackage{hyperref}
toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(FE)
library(forecast)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(psych)
library(systemfit)
library(msos)
library(fGarch)
data("portfolio_m")
```

# A. A. Estimation of GARCH processes

In this exercise, you have to model the time-varying volatility in different index series. Perform the following analysis based on daily indexes of your choice.

We use Daily S&P 500 index close price data from 1950 to 2015, obtained from the 'gt' package.

```{r}
library(gt)
close <- sp500$close #Daily S&P 500 index close from 1950 to 2015
r <- diff(log(close))
ts.plot(r)
ts.plot(r^2)
```

## 1. Analyze the dynamical properties of log returns and squared log returns. Suggest an appropriate AR-GARCH model and estimate it. Hint: you can use an R package, for example, the package ’fGarch’. Since the package ’fGarch’ allows for ARMA specification in the conditional mean, you can also fit the data by using ARMA-GARCH framework if you want.


```{r}
desc_r <- describe(r)[, c("n", "mean", "median","sd", "min", "max", "skew", "kurtosis")]
desc_r2 <- describe(r^2)[, c("n", "mean", "median","sd", "min", "max", "skew", "kurtosis")]
table <- rbind(desc_r,desc_r2)
rownames(table) <- c("r", "r^2")
kable(round(table,4))
```

```{r}
Acf(r, lag.max=20)
Pacf(r, lag.max=20)
Acf(r^2, lag.max=20)
Pacf(r^2, lag.max=20)
```
We can see that the log returns series $r_t=\mu_t+\epsilon_t$ has significant autocorrelation. So now we need to specify an appropriate $ARMA(p,q)$ model for the conditional mean function $\mu_t$.

```{r}
m1 <- auto.arima(r)
m1
```
We see that auto.arima() suggests an $ARMA(2,0)$ i.e. $AR(2)$ with a constant for the conditional mean function $\mu_t$. So now we want to test for ARCH effects. So we have the residuals $\epsilon_t=r_t-\mu_t$. We first see if they are serially uncorrelated

```{r}
epsilon = m1$residuals
Acf(epsilon, lag.max=20)
Box.test(epsilon, lag = 20, type = "Ljung-Box")
```
We can again see that the residuals are serially correlated, since we reject the null of the first 20 autocorrelations being zero. Now to check the time series properties of the squared mean correced returns $\epsilon_t^2$.

```{r}
Acf(epsilon^2)
Pacf(epsilon^2)
Box.test(epsilon^2, lag = 20, type = "Ljung-Box")
```

We can see that the squared mean corrected returns are not serially uncorrelated, which suggests ARCH effects. We see that there is a very high persistence, so we would need a $ARCH(p)$ model with a large $p$. To make it more parsimonious, we can make use of a $ARMA(2,0)-GARCH(1,1)$ model.

```{r}
m2 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, trace = F)
```

## 2. Test whether the model is able to capture the dynamical properties of the data and determine the optimal lag order by

### $\bullet$ plotting the original returns, the standardized residuals as well as the estimated conditional standard deviation,

```{r}
plot(m2, which = 1) #original returns
plot(m2, which = 9) #the standardized residuals
plot(m2, which = 2) #estimated conditional standard deviation
```

### $\bullet$ evaluating the autocorrelation functions of of the standardized residuals and squared standardized residuals,

```{r}
z_hat = residuals(m2, standardize = TRUE)
Acf(z_hat, lag.max=20)
```
It is hard to tell if there is much autocorrelation or not for the standardized residuals, since we have 2 spikes that are just barely significant. Lets check the squared standardized residuals

```{r}
Acf(z_hat^2, lag.max=20)
```
Now the ACF plot is a bit more clear, it does not indicate very strong autocorrelation, we have 2 barely significant spikes, but they are smaller this time.

### $\bullet$ computing Portmanteau statistics based on the standardized residuals,


Lets perform a Ljung-Box test for the standardized residuals with the null that the first 20 autocorrelations are zero.

```{r}
Box.test(z_hat, lag = 20, type = "Ljung-Box")
```

As we can see we can not reject even on a 10\% level that the standardized residuals are uncorrelated. Lets do the same for the squared standardized residuals

```{r}
Box.test(z_hat^2, lag = 20, type = "Ljung-Box")
```

Again we can not reject even on a 10\% level that the squared standardized residuals are uncorrelated.

### $\bullet$ testing for (G)ARCH effects of higher orders by applying ARCH-LM tests,

We now try other ARMA-GARCH models with higher GARCH orders.

```{r}
m3 <- garchFit(r ~ arma(2, 0) + garch(1, 2), data = r, trace = F)
m4 <- garchFit(r ~ arma(2, 0) + garch(2, 1), data = r, trace = F)
m5 <- garchFit(r ~ arma(2, 0) + garch(2, 2), data = r, trace = F)
m6 <- garchFit(r ~ arma(2, 0) + garch(2, 3), data = r, trace = F)
m7 <- garchFit(r ~ arma(2, 0) + garch(3, 2), data = r, trace = F)
m8 <- garchFit(r ~ arma(2, 0) + garch(3, 3), data = r, trace = F)
```
Now we get the LM Arch Test directly from the summary function (also the previous ljung box test) and information criteria for each model.

```{r}
summary(m2)
```
To not clutter, we just check the tests for each model and do not include it in the report.

```{r, include=FALSE, eval=FALSE}
summary(m2)
summary(m3)
summary(m4)
summary(m5)
summary(m6)
summary(m7)
summary(m8)
```
\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{LM Arch Test for standardized residuals}             \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{Statistic} & p-Value    \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{17.05822}  & 0.14742034 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{17.04670}  & 0.14784886 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{17.50144}  & 0.13168815 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{17.67835}  & 0.12581163 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{13.29505}  & 0.34796510 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{17.66287}  & 0.12631691 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{12.003996} & 0.44535875
\end{tabular}
\end{table}

Indeed m8 has the highest p-value, so that would maybe imply a ARMA(2,0)-GARCH(3,3) is more suitable. But since we have p-values for all models above 0.1, this means that we do not see any GARCH effects for the standardized residuals of any model. So our conclusion is that the LM arch tests says that an ARMA(2,0)-GARCH(1,1) is enough.

### $\bullet$ using information criteria.

We also get information criteria from the summary function.

\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{Information Criterion Statistics}                   \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{AIC}       & BIC       \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{-6.824967} & -6.822179 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{-6.824853} & -6.821600 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{-6.824857} & -6.821604 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{-6.824763} & -6.821045 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{-6.824752} & -6.820569 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{-6.824655} & -6.820472 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{-6.824873} & -6.820225
\end{tabular}
\end{table}

Furthermore the AIC/BIC values confirm that m2, i.e. the ARMA(2,0)-GARCH(1,1) has the lowest AIC and BIC values, so the information criteria confirm that the more parsimonious model is the best/enough.

Summarizing the ACF plots, the Portmanteau statistics, the ARCH-LM tests and information criteria (AIC and BIC) all point towards the conclusion that the model ARMA(2,0)-GARCH(1,1), i.e. m2, is able to capture the dynamical properties of the data. Therefore the optimal lag order would be 1 for p and q for the GARCH.

## 3. Compute the standardized residuals and test the series against normality using appropriate goodness-of-fit tests as well as QQ-plots. What do you find?

```{r}
z_hat = residuals(m2, standardize=TRUE)

vdata = z_hat
ik = 20
grids = 1:ik/ik

vq = pnorm(vdata)

ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("N(0,1)")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

The Chi square goodness of fit test clearly rejects the null that the standardized residuals $\hat{z}_t \sim N(0,1)$. Also we can see from the histogram that it does not look like a uniform distribution, which it should look like if the null is true.

```{r}
qqnorm(z_hat)
qqline(z_hat, col="red")
```

The qqplot also confirms that the standardized residuals are not normally distributed.


## 4. Re-estimate the model based on a t-distribution and re-evaluate the distributional properties of the resulting standardized residuals. Do you find the t-distribution to be more appropriate?

```{r}
m9 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, cond.dist = "std", trace = F)
```

