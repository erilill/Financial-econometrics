---
title: "Report for PS5"
author:
- Mark Becker, Erik Lillrank & Vilim Nedic
date: ''
output:
  pdf_document:
    keep_tex: true
  df_print: kable
  html_document:
    df_print: paged
  word_document: default
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{multirow}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{adjustbox}
  - \usepackage{hyperref}
toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(FE)
library(forecast)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(psych)
library(systemfit)
library(msos)
library(fGarch)
library(MASS)
data("portfolio_m")
```

# A. A. Estimation of GARCH processes

In this exercise, you have to model the time-varying volatility in different index series. Perform the following analysis based on daily indexes of your choice.

We use Daily S&P 500 index close price data from 1950 to 2015, obtained from the 'gt' package.

```{r}
library(gt)
close <- gt::sp500$close #Daily S&P 500 index close from 1950 to 2015
r <- diff(log(close))
ts.plot(r)
ts.plot(r^2)
```

## 1. Analyze the dynamical properties of log returns and squared log returns. Suggest an appropriate AR-GARCH model and estimate it. Hint: you can use an R package, for example, the package ’fGarch’. Since the package ’fGarch’ allows for ARMA specification in the conditional mean, you can also fit the data by using ARMA-GARCH framework if you want.


```{r}
Acf(r, lag.max=20)
Pacf(r, lag.max=20)
Acf(r^2, lag.max=20)
Pacf(r^2, lag.max=20)
```
We can see that the log returns series $r_t=\mu_t+\epsilon_t$ has significant autocorrelation. We also examine the degree of persistence in the volatility of the process, which we can see by the squared log returns. As we can see in the ACF and PACF plots, there is very much persistence. Now we need to specify an appropriate $ARMA(p,0)$ model for the conditional mean function $\mu_t$. We use the auto.arima() function to determine the appropriate $AR(p)$ model.

```{r}
m1 <- auto.arima(r, max.q=0)
m1
```
We see that auto.arima() suggests an $ARMA(2,0)$ i.e. $AR(2)$ with a constant for the conditional mean function $\mu_t$. So now we want to test for ARCH effects. So we have the residuals $\epsilon_t=r_t-\mu_t$. We first see if they are serially uncorrelated

```{r}
epsilon = m1$residuals
Acf(epsilon, lag.max=20)
Box.test(epsilon, lag = 20, type = "Ljung-Box")
```
We can again see that the residuals are serially correlated, since we reject the null of the first 20 autocorrelations being zero. Now to check the time series properties of the squared mean corrected returns $\epsilon_t^2$.

```{r}
Acf(epsilon^2)
Pacf(epsilon^2)
Box.test(epsilon^2, lag = 20, type = "Ljung-Box")
```

We can see that the squared mean corrected returns are not serially uncorrelated, which suggests ARCH effects. We see that there is a very high persistence, so we would need a $ARCH(p)$ model with a large $p$. To make it more parsimonious, we can make use of a $ARMA(2,0)-GARCH(1,1)$ model.

```{r}
m2 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, trace = F)
```

## 2. Test whether the model is able to capture the dynamical properties of the data and determine the optimal lag order by

### $\bullet$ plotting the original returns, the standardized residuals as well as the estimated conditional standard deviation,

```{r}
plot(m2, which = 1) #original returns
plot(m2, which = 9) #the standardized residuals
plot(m2, which = 2) #estimated conditional standard deviation
```

### $\bullet$ evaluating the autocorrelation functions of of the standardized residuals and squared standardized residuals,

```{r}
z_hat = residuals(m2, standardize = TRUE)
Acf(z_hat, lag.max=20)
```
It is hard to tell if there is much autocorrelation or not for the standardized residuals, since we have 2 spikes that are just barely significant. Lets check the squared standardized residuals

```{r}
Acf(z_hat^2, lag.max=20)
```
Now the ACF plot is a bit more clear, it does not indicate much autocorrelation, we have 2 barely significant spikes, but they are smaller this time.

### $\bullet$ computing Portmanteau statistics based on the standardized residuals,


Lets perform a Ljung-Box test for the standardized residuals with the null that the first 20 autocorrelations are zero.

```{r}
Box.test(z_hat, lag = 20, type = "Ljung-Box")
```

As we can see we can not reject even on a 10\% level that the standardized residuals are uncorrelated. Lets do the same for the squared standardized residuals

```{r}
Box.test(z_hat^2, lag = 20, type = "Ljung-Box")
```

Again we can not reject even on a 10\% level that the squared standardized residuals are uncorrelated.

### $\bullet$ testing for (G)ARCH effects of higher orders by applying ARCH-LM tests,

We now try other ARMA-GARCH models with higher GARCH orders.

```{r}
m3 <- garchFit(r ~ arma(2, 0) + garch(1, 2), data = r, trace = F)
m4 <- garchFit(r ~ arma(2, 0) + garch(2, 1), data = r, trace = F)
m5 <- garchFit(r ~ arma(2, 0) + garch(2, 2), data = r, trace = F)
m6 <- garchFit(r ~ arma(2, 0) + garch(2, 3), data = r, trace = F)
m7 <- garchFit(r ~ arma(2, 0) + garch(3, 2), data = r, trace = F)
m8 <- garchFit(r ~ arma(2, 0) + garch(3, 3), data = r, trace = F)
```
Now we get the LM Arch Test directly from the summary function (also the previous ljung box test) and information criteria for each model.

```{r}
summary(m2)
```
To not clutter, we just check the LM Arch tests for each model and do not include it in the report.

```{r, include=FALSE, eval=FALSE}
summary(m2)
summary(m3)
summary(m4)
summary(m5)
summary(m6)
summary(m7)
summary(m8)
```
\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{LM Arch Test for standardized residuals}             \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{Statistic} & p-Value    \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{17.05822}  & 0.14742034 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{17.04670}  & 0.14784886 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{17.50144}  & 0.13168815 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{17.67835}  & 0.12581163 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{13.29505}  & 0.34796510 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{17.66287}  & 0.12631691 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{12.003996} & 0.44535875
\end{tabular}
\end{table}

Indeed m8 has the highest p-value, so that would maybe imply a ARMA(2,0)-GARCH(3,3) is more suitable. But since we have p-values for all models above 0.1, this means that we do not see any GARCH effects for the standardized residuals of any model. So our conclusion is that the LM arch tests says that an ARMA(2,0)-GARCH(1,1) is enough.

### $\bullet$ using information criteria.

We also get information criteria from the summary function.

\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{Information Criterion Statistics}                   \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{AIC}       & BIC       \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{-6.824967} & -6.822179 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{-6.824853} & -6.821600 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{-6.824857} & -6.821604 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{-6.824763} & -6.821045 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{-6.824752} & -6.820569 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{-6.824655} & -6.820472 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{-6.824873} & -6.820225
\end{tabular}
\end{table}

Furthermore the AIC/BIC values confirm that m2, i.e. the ARMA(2,0)-GARCH(1,1) has the lowest AIC and BIC values, so the information criteria confirm that the more parsimonious model is the best/enough.

Summarizing the ACF plots, the Portmanteau statistics, the ARCH-LM tests and information criteria (AIC and BIC) all point towards the conclusion that the model ARMA(2,0)-GARCH(1,1), i.e. m2, is able to capture the dynamical properties of the data. Therefore the optimal lag order would be 1 for p and q for the GARCH, and 2 for p and 0 for q in the ARMA part.

## 3. Compute the standardized residuals and test the series against normality using appropriate goodness-of-fit tests as well as QQ-plots. What do you find?

```{r}
z_hat = residuals(m2, standardize=TRUE)

vdata = z_hat
ik = 20
grids = 1:ik/ik

vq = pnorm(vdata)
hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

The Chi square goodness of fit test clearly rejects the null that the standardized residuals $\hat{z}_t \sim N(0,1)$. Also we can see from the histogram that it does not look like a uniform distribution, which it should look like if the null is true.

```{r}
qqnorm(z_hat)
qqline(z_hat, col="red")
```

The qqplot also confirms that the standardized residuals are not normally distributed.


## 4. Re-estimate the model based on a t-distribution and re-evaluate the distributional properties of the resulting standardized residuals. Do you find the t-distribution to be more appropriate?

```{r}
m9 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, cond.dist = "std", trace = F)
```
If we look at the summary now, the estimated shape parameter, i.e. the estimated degrees of freedom is 6.7. So we try the standardized residuals against a t distribution with 7 degrees of freedom.

```{r}
z_hat_t = residuals(m9,standardize=TRUE)

vdata = z_hat_t
ik = 20
grids = 1:ik/ik

df = 7
ndata = vdata*sqrt(df/(df-2))
vq = pt(ndata,df=7); hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```
Still quite a high test statistic of 59, but much smaller than for the normal distribution where the test statistic was 323. So we can not accept the null this time either, but we can conclude that the test statistic suggests that the t distribution has a better goodness of fit than the normal distribution. Lets look at the qqplot.

```{r}
dataframe = data.frame(z_hat_t)

df = 7
tmp = ggplot(dataframe, aes(sample=z_hat_t*sqrt(df/(df-2))))
tmp + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red")
```
It seems like the t distribution is more appropriate than the normal distribution.

## 5. Redo the analysis above using return series based on higher aggregation levels.

Now we use monthly sp500 data from 1950-01 to 2019-12 from the bbdetection package.

```{r}
library(bbdetection)
data <- sp500m
P <- as.vector(data$SP500) # monthly closing prices
r <- diff(log(P))
ts.plot(r)
ts.plot(r^2)
```

```{r}
Acf(r, lag.max=12)
Pacf(r, lag.max=12)
Acf(r^2, lag.max=12)
Pacf(r^2, lag.max=12)
```

```{r}
m10 <- auto.arima(r)
m10

epsilon = m10$residuals
Acf(epsilon, lag.max=12)
Box.test(epsilon, lag = 12, type = "Ljung-Box")

Acf(epsilon^2)
Pacf(epsilon^2)
Box.test(epsilon^2, lag = 12, type = "Ljung-Box")

m11 <- garchFit(r ~ arma(1, 1) + garch(1, 1), data = r, trace = F)
```

```{r}
plot(m11, which = 1) #original returns
plot(m11, which = 9) #the standardized residuals
plot(m11, which = 2) #estimated conditional standard deviation

z_hat = residuals(m2, standardize = TRUE)
Acf(z_hat, lag.max=12)
Acf(z_hat^2, lag.max=12)
Box.test(z_hat, lag = 12, type = "Ljung-Box")
Box.test(z_hat^2, lag = 12, type = "Ljung-Box")
```

# B. Estimating Value-at-Risk

This exercise deals with the estimation of the Value-at-Risk (VaR) based on alternative underlying models. Use the data sets ”DJ_d” (Dow Jones daily) and ”DJ_w” (Dow Jones weekly) containing daily and weekly observations of the Dow Jones index.

```{r}
r_d <- DJ_d$r_Dow_Jones
r_w <- DJ_w$r_close
```

Compute the Value-at-Risk of a position V = 1 for different horizons using

## $\bullet$ the unconditional moments of a normal distribution

```{r}
VaR_normal <- function(mu,sigma2,p,h,V){
  
  VaR <- list()
  phi_inv_p <- qnorm(p)
  
  for (i in 1:length(phi_inv_p)){
    VaR[[i]] <- V*(exp(phi_inv_p[i]*sqrt(h*sigma2)+h*mu)-1)
  }
  return(VaR)
}

#slide 41 replication
VaR_normal(mu=0.001,sigma2=0.015^2,p=0.01,h=c(1,10,30),V=10)
VaR_normal(mu=0.001,sigma2=0.016^2,p=0.01,h=c(1,10,30),V=10)
```
For the daily data and horizons 1, 10 and 30, V=1 and using the mentioned probabilities.

```{r}
## daily
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,10,30)
V <- 1

VaR_normal(mean(r_d),var(r_d),p,h,V)
```

For the weekly data and horizons 1, 4 and 8, V=1 and using the mentioned probabilities.

```{r}
## weekly
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,4,8)
V <- 1

VaR_normal(mean(r_w),var(r_w),p,h,V)
```
## $\bullet$ the unconditional moments of a t-distribution

```{r}
VaR_t <- function(mu, sigma2, p, h, V, nu) {
  VaR <- list()
  t_inv_p <- qt(p, df = nu)
  
  for (i in 1:length(t_inv_p)) {
    VaR[[i]] <- V * (exp(t_inv_p[i] * sqrt(h * sigma2) + h * mu) - 1)
  }
  return(VaR)
}
```

For the daily data and horizons 1, 10 and 30, V=1 and using the mentioned probabilities.

```{r}
## daily
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,10,30)
V <- 1
nu <- 5

VaR_t(mean(r_d),var(r_d),p,h,V, nu)
```

For the weekly data and horizons 1, 4 and 8, V=1 and using the mentioned probabilities.

```{r}
## weekly
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,4,8)
V <- 1
nu <- 5

VaR_t(mean(r_w),var(r_w),p,h,V, nu)
```

## $\bullet$ a Gaussian GARCH(1,1) with constant conditional mean function,

```{r}
m12 <- garchFit(r_d ~ garch(1, 1), cond.dist ="norm", data = r_d, trace = F)
m13 <- garchFit(r_w ~ garch(1, 1), cond.dist ="norm", data = r_w, trace = F)
```

First for daily data and horizons 1,10,30

```{r}
VaR_normal_ARMA_GARCH <- function(model,p,h,V){
  
  pred <- fGarch::predict(model,n.ahead=tail(h,1))[h,]
  mupred <- pred$meanForecast
  sigma2pred <- pred$standardDeviation^2
  
  VaR <- list()
  phi_inv_p <- qnorm(p)
  
  for (i in 1:length(phi_inv_p)){
    VaR[[i]] <- V*(exp(phi_inv_p[i]*sqrt(sigma2pred)+mupred)-1)
  }
  return(VaR)
}

p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
V <- 1

h <- c(1,10,30)
VaR_normal_ARMA_GARCH(m12,p,h,V) #daily
```

And for weekly with horizons 1,4,8.

```{r}
h <- c(1,4,8)
VaR_normal_ARMA_GARCH(m13,p,h,V) #weekly
```

## $\bullet$ a GARCH(1,1) model based on the t-distribution with constant conditional mean function.

```{r}
m14 <- garchFit(r_d ~ garch(1, 1), cond.dist ="std", data = r_d, trace = F)
m15 <- garchFit(r_w ~ garch(1, 1), cond.dist ="std", data = r_w, trace = F)
```