---
title: "Report for PS5"
author:
- Mark Becker, Erik Lillrank & Vilim Nedic
date: ''
output:
  pdf_document:
    keep_tex: true
  df_print: kable
  html_document:
    df_print: paged
  word_document: default
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{multirow}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{adjustbox}
  - \usepackage{hyperref}
toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(FE)
library(forecast)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(psych)
library(systemfit)
library(msos)
library(fGarch)
library(MASS)
data("portfolio_m")
```

# A. A. Estimation of GARCH processes

In this exercise, you have to model the time-varying volatility in different index series. Perform the following analysis based on daily indexes of your choice.

We use Daily S&P 500 index close price data from 1950 to 2015, obtained from the 'gt' package.

```{r}
library(gt)
close <- gt::sp500$close #Daily S&P 500 index close from 1950 to 2015
r <- diff(log(close))
ts.plot(r)
ts.plot(r^2)
```

## 1. Analyze the dynamical properties of log returns and squared log returns. Suggest an appropriate AR-GARCH model and estimate it. Hint: you can use an R package, for example, the package ’fGarch’. Since the package ’fGarch’ allows for ARMA specification in the conditional mean, you can also fit the data by using ARMA-GARCH framework if you want.


```{r}
Acf(r, lag.max=20)
Pacf(r, lag.max=20)
Acf(r^2, lag.max=20)
Pacf(r^2, lag.max=20)
```
We can see that the log returns series $r_t=\mu_t+\epsilon_t$ has significant autocorrelation. We also examine the degree of persistence in the volatility of the process, which we can see by the squared log returns. As we can see in the ACF and PACF plots, there is very much persistence. Now we need to specify an appropriate $ARMA(p,0)$ model for the conditional mean function $\mu_t$. We use the auto.arima() function to determine the appropriate $AR(p)$ model.

```{r}
m1 <- auto.arima(r, max.q=0)
m1
```
We see that auto.arima() suggests an $ARMA(2,0)$ i.e. $AR(2)$ with a constant for the conditional mean function $\mu_t$. So now we want to test for ARCH effects. So we have the residuals $\epsilon_t=r_t-\mu_t$. We first see if they are serially uncorrelated

```{r}
epsilon = m1$residuals
Acf(epsilon, lag.max=20)
Box.test(epsilon, lag = 20, type = "Ljung-Box")
```
We can again see that the residuals are serially correlated, since we reject the null of the first 20 autocorrelations being zero. Now to check the time series properties of the squared mean corrected returns $\epsilon_t^2$.

```{r}
Acf(epsilon^2)
Pacf(epsilon^2)
Box.test(epsilon^2, lag = 20, type = "Ljung-Box")
```

We can see that the squared mean corrected returns are not serially uncorrelated, which suggests ARCH effects. We see that there is a very high persistence, so we would need a $ARCH(p)$ model with a large $p$. To make it more parsimonious, we can make use of a $ARMA(2,0)-GARCH(1,1)$ model.

```{r}
m2 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, trace = F)
```

## 2. Test whether the model is able to capture the dynamical properties of the data and determine the optimal lag order by

### $\bullet$ plotting the original returns, the standardized residuals as well as the estimated conditional standard deviation,

```{r}
plot(m2, which = 1) #original returns
plot(m2, which = 9) #the standardized residuals
plot(m2, which = 2) #estimated conditional standard deviation
```
The standardized residuals look like white noise, which suggests a good model fit.

### $\bullet$ evaluating the autocorrelation functions of of the standardized residuals and squared standardized residuals,

```{r}
z_hat = residuals(m2, standardize = TRUE)
Acf(z_hat, lag.max=20)
```
It is hard to tell if there is much autocorrelation or not for the standardized residuals, since we have 2 spikes that are just barely significant. Lets check the squared standardized residuals

```{r}
Acf(z_hat^2, lag.max=20)
```
Now the ACF plot is a bit more clear, it does not indicate much autocorrelation, we have 2 barely significant spikes, but they are smaller this time.

### $\bullet$ computing Portmanteau statistics based on the standardized residuals,


Lets perform a Ljung-Box test for the standardized residuals with the null that the first 20 autocorrelations are zero.

```{r}
Box.test(z_hat, lag = 20, type = "Ljung-Box")
```

As we can see we can not reject even on a 10\% level that the standardized residuals are uncorrelated. Lets do the same for the squared standardized residuals

```{r}
Box.test(z_hat^2, lag = 20, type = "Ljung-Box")
```

Again we can not reject even on a 10\% level that the squared standardized residuals are uncorrelated.

### $\bullet$ testing for (G)ARCH effects of higher orders by applying ARCH-LM tests,

We now try other ARMA-GARCH models with higher GARCH orders.

```{r}
m3 <- garchFit(r ~ arma(2, 0) + garch(1, 2), data = r, trace = F)
m4 <- garchFit(r ~ arma(2, 0) + garch(2, 1), data = r, trace = F)
m5 <- garchFit(r ~ arma(2, 0) + garch(2, 2), data = r, trace = F)
m6 <- garchFit(r ~ arma(2, 0) + garch(2, 3), data = r, trace = F)
m7 <- garchFit(r ~ arma(2, 0) + garch(3, 2), data = r, trace = F)
m8 <- garchFit(r ~ arma(2, 0) + garch(3, 3), data = r, trace = F)
```
Now we get the LM Arch Test directly from the summary function (also the previous ljung box test) and information criteria for each model.

```{r}
summary(m2)
```
To not clutter, we just check the LM Arch tests for each model and do not include it in the report.

```{r, include=FALSE, eval=FALSE}
summary(m2)
summary(m3)
summary(m4)
summary(m5)
summary(m6)
summary(m7)
summary(m8)
```
\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{LM Arch Test for standardized residuals}             \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{Statistic} & p-Value    \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{17.05822}  & 0.14742034 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{17.04670}  & 0.14784886 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{17.50144}  & 0.13168815 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{17.67835}  & 0.12581163 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{13.29505}  & 0.34796510 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{17.66287}  & 0.12631691 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{12.003996} & 0.44535875
\end{tabular}
\end{table}

Indeed m8 has the highest p-value, so that would maybe imply a ARMA(2,0)-GARCH(3,3) is more suitable. But since we have p-values for all models above 0.1, this means that we do not see any GARCH effects for the standardized residuals of any model. So our conclusion is that the LM arch tests says that an ARMA(2,0)-GARCH(1,1) is good enough.

### $\bullet$ using information criteria.

We also get information criteria from the summary function.

\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{Information Criterion Statistics}                   \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{AIC}       & BIC       \\ \hline
\multicolumn{1}{|c|}{m2}    & \multicolumn{1}{c|}{-6.824967} & -6.822179 \\
\multicolumn{1}{|c|}{m3}    & \multicolumn{1}{c|}{-6.824853} & -6.821600 \\
\multicolumn{1}{|c|}{m4}    & \multicolumn{1}{c|}{-6.824857} & -6.821604 \\
\multicolumn{1}{|c|}{m5}    & \multicolumn{1}{c|}{-6.824763} & -6.821045 \\
\multicolumn{1}{|c|}{m6}    & \multicolumn{1}{c|}{-6.824752} & -6.820569 \\
\multicolumn{1}{|c|}{m7}    & \multicolumn{1}{c|}{-6.824655} & -6.820472 \\
\multicolumn{1}{|c|}{m8}    & \multicolumn{1}{c|}{-6.824873} & -6.820225
\end{tabular}
\end{table}

Furthermore the AIC/BIC values confirm that m2, i.e. the ARMA(2,0)-GARCH(1,1) has the lowest AIC and BIC values, so the information criteria confirm that the more parsimonious model is the best/enough.

Summarizing the ACF plots, the Portmanteau statistics, the ARCH-LM tests and information criteria (AIC and BIC) all point towards the conclusion that the model ARMA(2,0)-GARCH(1,1), i.e. m2, is able to capture the dynamical properties of the data. Therefore the optimal lag order would be 1 for p and q for the GARCH, and 2 for p and 0 for q in the ARMA part.

## 3. Compute the standardized residuals and test the series against normality using appropriate goodness-of-fit tests as well as QQ-plots. What do you find?

```{r}
z_hat = residuals(m2, standardize=TRUE)

vdata = z_hat
ik = 20
grids = 1:ik/ik

vq = pnorm(vdata)
hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

The Chi square goodness of fit test clearly rejects the null that the standardized residuals $\hat{z}_t \sim N(0,1)$. Also we can see from the histogram that it does not look like a uniform distribution, which it should look like if the null is true.

```{r}
qqnorm(z_hat)
qqline(z_hat, col="red")
```

The qqplot also confirms that the standardized residuals are not normally distributed.


## 4. Re-estimate the model based on a t-distribution and re-evaluate the distributional properties of the resulting standardized residuals. Do you find the t-distribution to be more appropriate?

```{r}
m9 <- garchFit(r ~ arma(2, 0) + garch(1, 1), data = r, cond.dist = "std", trace = F)
```
If we look at the summary now, the estimated shape parameter, i.e. the estimated degrees of freedom is 6.7. So we try the standardized residuals against a t distribution with 7 degrees of freedom.

```{r}
z_hat_t = residuals(m9,standardize=TRUE)

vdata = z_hat_t
ik = 20
grids = 1:ik/ik

df = 7
ndata = vdata*sqrt(df/(df-2))
vq = pt(ndata,df=7); hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```
Still quite a high test statistic of 59, but much smaller than for the normal distribution where the test statistic was 323. So we can not accept the null this time either, but we can conclude that the test statistic suggests that the t distribution has a better goodness of fit than the normal distribution. Lets look at the qqplot.

```{r}
dataframe = data.frame(z_hat_t)

df = 7
tmp = ggplot(dataframe, aes(sample=z_hat_t*sqrt(df/(df-2))))
tmp + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red")
```
It seems like the t distribution is more appropriate than the normal distribution.

## 5. Redo the analysis above using return series based on higher aggregation levels.

Now we use monthly sp500 data from 1950-01 to 2019-12 from the bbdetection package.

```{r}
library(bbdetection)
data <- sp500m
P <- as.vector(data$SP500) # monthly closing prices
r <- diff(log(P))
ts.plot(r)
ts.plot(r^2)
```

First we analyze the dynamical properties of the log returns and squared log returns.

```{r}
Acf(r, lag.max=12)
Pacf(r, lag.max=12)
Acf(r^2, lag.max=12)
Pacf(r^2, lag.max=12)
```
We can see that there is not much persistence in the log returns, but there is persistence in the squared log returns. To specify the mean equation we use auto.arima again

```{r}
m10 <- auto.arima(r, max.q=0)
m10
```
We can see that auto.arima suggests that the returns are white noise. We use a t-test to see if the mean is different from zero

```{r}
t.test(r)
```
The t test suggests the mean is different from zero, so we get our mean-corrected returns by simply

```{r}
epsilon <- r-mean(r)
```

This means that we have a constant conditional mean function. Now we look at the ACF plots and do box tests for the mean corrected returns and squared mean corrected returns

```{r}
Acf(epsilon, lag.max=12)
Pacf(epsilon, lag.max=12)
Box.test(epsilon, lag = 12, type = "Ljung-Box")

Acf(epsilon^2, lag.max=12)
Pacf(epsilon^2, lag.max=12)
Box.test(epsilon^2, lag = 12, type = "Ljung-Box")
```
We can see that the ACF/PACF plots suggest litte to no autocorrelation in the mean-corrected returns. The Ljung-Box test furthermore does not reject the null of no autocorrelation in the mean-corrected returns. However the ACF and PACF plots for the squared mean corrected returns indicate significant autocorrelation. As well as the Ljung-Box tests which rejects the null of no autocorrelation in the squared mean corrected (log) returns, which indicates that there are ARCH effects present. Now we again consider a $GARCH(1,1)$, but the PACF plot suggests maybe an $ARCH(3)$ would be okay also, but still we do $GARCH(1,1)$. To fit a (gaussian by default) $GARCH(1,1)$ with a constant conditional mean function we simply do

```{r}
m11 <- garchFit(r ~ garch(1, 1), data = r, trace = F)
```

Now we test whether the model is able to capture the dynamical properties of the data and determine the optimal lag order.

First we plot the original returns, the standardized residuals, as well as the estimated conditional standard deviation.

```{r}
plot(m11, which = 1) #original returns
plot(m11, which = 9) #the standardized residuals
plot(m11, which = 2) #estimated conditional standard deviation
```
It is hard to tell if the standardized residuals are white noise. More analysis is needed. So we evaluate the autocorrelation functions of the standardized reisudals and squared standardized residuals.

```{r}
z_hat = residuals(m11, standardize = TRUE)
Acf(z_hat, lag.max=12)
Pacf(z_hat, lag.max=12)
Acf(z_hat^2, lag.max=12)
Pacf(z_hat^2, lag.max=12)
```
The ACF/PACF plots suggests there is no autocorrelation in the standardized or in the squared standardized residuals. Now we compute Portmanteau statistics based on the standardized residuals.

```{r}
Box.test(z_hat, lag = 12, type = "Ljung-Box")
Box.test(z_hat^2, lag = 12, type = "Ljung-Box")
```

We do not reject the null that there is no autocorrelation in the standardized residuals, and we also do not reject the null of no autocorrelation in the squared standardized residuals. So we do have evidence that the model is able to capture the dynamical properties of the data. Now to determine optimal lag order. Lets not check so many different models this time. So we test for GARCH effects of higher orders

```{r}
m12 <- garchFit(r ~ garch(1, 2), data = r, trace = F)
m13 <- garchFit(r ~ garch(2, 1), data = r, trace = F)
m14 <- garchFit(r ~ garch(2, 2), data = r, trace = F)
```

\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{LM Arch Test for standardized residuals}             \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{Statistic} & p-Value    \\ \hline
\multicolumn{1}{|c|}{m11}    & \multicolumn{1}{c|}{5.3125715}  & 0.9467159 \\
\multicolumn{1}{|c|}{m12}    & \multicolumn{1}{c|}{5.3197430}  & 0.946438 \\
\multicolumn{1}{|c|}{m13}    & \multicolumn{1}{c|}{5.6930511}  & 0.9307579 \\
\multicolumn{1}{|c|}{14}    & \multicolumn{1}{c|}{5.6930058}  & 0.9307599
\end{tabular}
\end{table}

The p-values are all very high, so we do not reject the null of no GARCH effects for all models. This suggests that the $GARCH(1,1)$ is a good enough lag order. Let us look at the AIC/BIC values

\begin{table}[]
\begin{tabular}{|ccc|}
\multicolumn{3}{|c|}{Information Criterion Statistics}                   \\ \hline
\multicolumn{1}{|c|}{model} & \multicolumn{1}{c|}{AIC}       & BIC       \\ \hline
\multicolumn{1}{|c|}{m11}    & \multicolumn{1}{c|}{-3.602803} & -3.580242 \\
\multicolumn{1}{|c|}{m12}    & \multicolumn{1}{c|}{-3.600297} & -3.572096 \\
\multicolumn{1}{|c|}{m13}    & \multicolumn{1}{c|}{-3.602829} & -3.574628 \\
\multicolumn{1}{|c|}{m14}    & \multicolumn{1}{c|}{-3.600446} & -3.566604 \\
\end{tabular}
\end{table}

For the AIC, it suggests that $GARCH(2,1)$ is slightly slightly better, with $GARCHH(1,1)$ at the second place. Although BIC suggests $GARCH(1,1)$ is the best model. This suggests that the 1,1 order for the $GARCH(1,1)$ is good enough, and if BIC decides it is actually the optimal lag order.

# B. Estimating Value-at-Risk

This exercise deals with the estimation of the Value-at-Risk (VaR) based on alternative underlying models. Use the data sets ”DJ_d” (Dow Jones daily) and ”DJ_w” (Dow Jones weekly) containing daily and weekly observations of the Dow Jones index.

```{r}
r_d <- DJ_d$r_Dow_Jones
r_w <- DJ_w$r_close
```

Compute the Value-at-Risk of a position V = 1 for different horizons using

## $\bullet$ the unconditional moments of a normal distribution

```{r}
VaR_normal <- function(mu,sigma2,p,h,V){
  
  VaR <- list()
  phi_inv_p <- qnorm(p)
  
  for (i in 1:length(phi_inv_p)){
    VaR[[i]] <- V*(exp(phi_inv_p[i]*sqrt(h*sigma2)+h*mu)-1)
  }
  return(VaR)
}

#slide 41 replication
VaR_normal(mu=0.001,sigma2=0.015^2,p=0.01,h=c(1,10,30),V=10)
VaR_normal(mu=0.001,sigma2=0.016^2,p=0.01,h=c(1,10,30),V=10)
```
For the daily data and horizons 1, 10 and 30, V=1 and using the mentioned probabilities.

For longer horizons lead to larger VaR as expectation and uncertainty increases with time. A higher variance increases the dispersion of potential returns and thus increasing VaR. 

```{r}
## daily
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,10,30)
V <- 1

res <- VaR_normal(mean(r_d),var(r_d),p,h,V)
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Daily from unconditional normal.",
  digits = 3
)
```

```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), # Select columns for Horizon values
    names_to = "Horizon",          # New column to indicate horizon type
    values_to = "VaR"              # New column to hold the values
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + # Optionally add points
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Daily",
    subtitle = "Unconditional moments of normal distribution",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```

For the weekly data and horizons 1, 4 and 8, V=1 and using the mentioned probabilities.

```{r}
## weekly
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,4,8)
V <- 1

res <- VaR_normal(mean(r_w),var(r_w),p,h,V)
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=4)", "Horizon (h=8)", "V"),
  caption = "Value-at-Risk Estimates for DJ Weekly from unconditional normal.",
  digits = 3
)
```


```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), # Select columns for Horizon values
    names_to = "Horizon",          # New column to indicate horizon type
    values_to = "VaR"              # New column to hold the values
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + # Optionally add points
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Weekly",
    subtitle = "Unconditional moments of normal distribution",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```



## $\bullet$ the unconditional moments of a t-distribution

```{r}
VaR_t <- function(mu, sigma2, p, h, V, nu) {
  VaR <- list()
  t_inv_p <- qt(p, df = nu)
  
  for (i in 1:length(t_inv_p)) {
    VaR[[i]] <- V * (exp(t_inv_p[i] * sqrt(h * sigma2) + h * mu) - 1)
  }
  return(VaR)
}
```

For the daily data and horizons 1, 10 and 30, V=1 and using the mentioned probabilities.

```{r}
## daily
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,10,30)
V <- 1
nu <- 5

res <- VaR_t(mean(r_d),var(r_d),p,h,V, nu)
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p)) 
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Daily from unconditional t-distributed moments",
  digits = 3
)
```

```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",          
    values_to = "VaR"              
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Daily",
    subtitle = "Unconditional t-distributed moments",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```



For the weekly data and horizons 1, 4 and 8, V=1 and using the mentioned probabilities.

```{r}
## weekly
p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
h <- c(1,4,8)
V <- 1
nu <- 5

res <- VaR_t(mean(r_w),var(r_w),p,h,V, nu)
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Weekly from unconditional t-distributed moments",
  digits = 3
)
```


```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",          
    values_to = "VaR"              
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Weekly",
    subtitle = "Unconditional t-distributed moments",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```

## $\bullet$ a Gaussian GARCH(1,1) with constant conditional mean function,

```{r}
m12 <- garchFit(r_d ~ garch(1, 1), cond.dist ="norm", data = r_d, trace = F)
m13 <- garchFit(r_w ~ garch(1, 1), cond.dist ="norm", data = r_w, trace = F)
```

First for daily data and horizons 1,10,30

```{r}
VaR_ARMA_GARCH <- function(model,p,h,V){
  
  pred <- fGarch::predict(model,n.ahead=tail(h,1))[h,]
  mupred <- pred$meanForecast
  sigma2pred <- pred$standardDeviation^2
  
  VaR <- list()
  phi_inv_p <- qnorm(p)
  
  for (i in 1:length(phi_inv_p)){
    VaR[[i]] <- V*(exp(phi_inv_p[i]*sqrt(sigma2pred)+mupred)-1)
  }
  return(VaR)
}

p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
V <- 1
h <- c(1,10,30)
res <- VaR_ARMA_GARCH(m12,p,h,V) #daily
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Daily from Gaussian GARCH(1,1) with constant conditional mean.",
  digits = 3
)
```


```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",         
    values_to = "VaR"             
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Daily",
    subtitle = "Gaussian GARCH(1,1) with constant conditional mean",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```

And for weekly with horizons 1,4,8.

```{r}
h <- c(1,4,8)
res <- VaR_ARMA_GARCH(m13,p,h,V) #weekly
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Weekly from Gaussian GARCH(1,1) with constant conditional mean.",
  digits = 3
)
```


```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",         
    values_to = "VaR"             
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Weekly",
    subtitle = "Gaussian GARCH(1,1) with constant conditional mean",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```

## $\bullet$ a GARCH(1,1) model based on the t-distribution with constant conditional mean function.

```{r}
m14 <- garchFit(r_d ~ garch(1, 1), cond.dist ="std", data = r_d, trace = F)
m15 <- garchFit(r_w ~ garch(1, 1), cond.dist ="std", data = r_w, trace = F)

p <- c(0.1, 0.05, 0.01, 0.005, 0.001)
V <- 1
h <- c(1,10,30)
res <- VaR_ARMA_GARCH(m14,p,h,V) #daily
```


```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Daily from GARCH(1,1) model based on the t-distribution with constant conditional mean function.",
  digits = 3
)
```

```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",         
    values_to = "VaR"             
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Daily",
    subtitle = "GARCH(1,1) model based on the t-distribution with constant conditional mean function",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```

```{r}
h <- c(1,4,8)
res <- VaR_ARMA_GARCH(m15,p,h,V) #weekly
```

```{r}
VaR_df <- data.frame(
  Probability = c(0.1, 0.05, 0.01, 0.005, 0.001),
  Horizon_1 = sapply(res, function(x) x[1]),
  Horizon_10 = sapply(res, function(x) x[2]),
  Horizon_30 = sapply(res, function(x) x[3]), 
  V = rep(V, length(p))
)

kable(
  VaR_df,
  col.names = c("Probability", "Horizon (h=1)", "Horizon (h=10)", "Horizon (h=30)", "V"),
  caption = "Value-at-Risk Estimates for DJ Weekly from GARCH(1,1) model based on the t-distribution with constant conditional mean function.",
  digits = 3
)
```


```{r}
df_long <- VaR_df %>%
  pivot_longer(
    cols = starts_with("Horizon"), 
    names_to = "Horizon",         
    values_to = "VaR"             
  )

ggplot(df_long, aes(x = Horizon, y = VaR, group = Probability, color = as.factor(Probability))) +
  geom_line(size = 1) +
  geom_point(size = 2) + 
  labs(
    title = "Value-at-Risk for Different Horizons and Probabilities DJ Weekly",
    subtitle = "GARCH(1,1) model based on the t-distribution with constant conditional mean function",
    x = "Horizon",
    y = "VaR",
    color = "Probability"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom"
  )
```


