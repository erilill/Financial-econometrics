---
title: "Report for PS 1"
author:
- Mark Becker, Erik Lillrank & Vilim Nedic
date: ''
output:
  pdf_document:
    keep_tex: true
  df_print: kable
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{float}
toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(FE)
library(psych)
library(forecast)
library(ggplot2)
library(patchwork)
library(knitr)
data("DJ_w")
data("DJ_d")
```

#  Statistical Properties of Asset Returns

## A. Distributional properties of Dow Jones index returns

### 1. Plot the series of log returns and compute descriptive statistics. Which distributional properties do you find? Do you find differences between daily and weekly data?

```{r, fig.height=4, fig.width=6, fig.align='center', fig.cap= "An example Figure.",echo=F}
p1 <- ggplot(DJ_d, aes(y=r_Dow_Jones,x=1:nrow(DJ_d))) + geom_line() +
  labs(x='time horizon',y='daily log return')

p2 <- ggplot(DJ_w, aes(y=r_close,x=1:nrow(DJ_w))) + geom_line() +
  labs(x='time horizon',y='weekly log return')

p1/p2
```

Let us look at some descriptive statistics.
```{r}
rbind(describe(DJ_d$r_Dow_Jones),describe(DJ_w$r_close))
```

Both series have means of zero. Daily (log) returns has standard deviation 0.01 and weekly (log) returns 0.03. Daily returns has a skewness of -0.96 and Weekly returns has a skewness of  -1.09. So both are skewed, although the weekly returns are a bit more skewed (which we can also see by looking at the differences of absolute values of the min and max for both series). Daily returns have a kurtosis of 36.6 and weekly a kurtosis of 15.06. So the largest difference between the daily and the weekly data is that the daily returns have much fatter tails than the weekly returns, as it has more than double the kurtosis. If we compare both distributions to a baseline, say a normal distribution, we know all normal distributions have skewness of 0 and kurtosis of 3. Since we have for both series skewness values quite far from 0 and kurtosis values which are much larger than 3, this is strong evidence that the returns are not normally distributed. We can further investigate which distribution seems most likely by producing QQ plots.


### 2. Evaluate the empirical distributions of the index log returns using quantile-quantile plots (QQ-plots). Test the empirical distribution against

#### a) a normal distribution (with same mean and variance)

```{r ndist}
ndistd <- ggplot(DJ_d, aes(sample=r_Dow_Jones))
ndistd + geom_qq(distribution=stats::qnorm, dparams = list(mean = mean(DJ_d$r_Dow_Jones), sd = sd(DJ_d$r_Dow_Jones))) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red") +
  labs(y = "Daily returns")
qqnorm(DJ_d$r_Dow_Jones)

ndistw <- ggplot(DJ_w, aes(sample=r_close))
ndistw + geom_qq(distribution=stats::qnorm, dparams = list(mean = mean(DJ_w$r_close), sd = sd(DJ_w$r_close))) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red") +
  labs(y = "Weekly returns")
qqnorm(DJ_w$r_close)
```
The first thing we observe when studying above plots are that the line intersects the data in the middle quite good. When it comes to the tails (lower left corner and upper right corner), we observe that the normal distribution captures these observations badly. This would indicate on a heavier-tailed data than the normal distribution would show. This finding is in accordance with other findings when studying econometric data as these are often more heavy-tailed.

#### b) alternative tn-distributions with n degrees of freedom.

 
```{r tdist}
df = 3
tdistd = ggplot(DJ_d, aes(sample=r_Dow_Jones/sd(r_Dow_Jones)*sqrt(df/(df-2))))
tdistd + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red") + 
  labs(y = "Daily returns")
qqplot(rt(length(DJ_d$r_Dow_Jones),df=3),DJ_d$r_Dow_Jones)

tdistw = ggplot(DJ_w, aes(sample=r_close/sd(r_close)*sqrt(df/(df-2))))
tdistw + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red") + 
  labs(y = "Weekly returns")
qqplot(rt(length(DJ_w$r_close),df=3),DJ_w$r_close)
```


A visual analysis of the QQ-plots show that both empirical distribution could follow a t-distribution with 3 degrees of freedom. Once again we observe that the t-distribution have a hard time capturing these heavier tails but it captures it much better than the normal could. We still observe some limited discrepancy in the lower left corner where weekly returns show one outlier while daily returns show two outliers. In the upper right corner we observe that the discrepancy between the theoretical distribution and the underlying distribution is bigger. There is especially one outlier that is poorly captured. This outlier and some other extreme observations are closer to the theoretical distribution when studying the daily returns compared to the weekly returns as the distance to the theoretical distribution is smaller in the daily returns. 



### 3. Compute a $\chi^2$-goodness-of-fit test against a standard normal, $t_{df}$ and mixture of normals.

We test the null of three different distributions. First a standard normal, then a $t_3$, and then a mixture of normals, where the first normal is standard normal, and the second is $N(0,\sigma_2^2)$, and the mixture is decided by the mixing probability $\alpha$.

```{r,include=FALSE}
vdata = DJ_d$r_Dow_Jones
#vdata = DJ_w$r_close
vdata = (vdata - mean(vdata))/sd(vdata)
ik = 20
grids = 1:ik/ik
```

```{r,include=FALSE}
vq = pnorm(vdata)

p1 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("N(0,1)")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))

```

```{r,include=FALSE}
df = 3
ndata = vdata*sqrt(df/(df-2))
vq = pt(ndata,df=3)

p2 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("t_3")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

```{r,include=FALSE}
func <- function(vx){
  alpha = vx[1]
  sigma = vx[2]
  ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)
  
  vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma)
  vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
  vn = c(vn[1],diff(vn))
  return(sum((vn-length(vdata)/ik)**2/(length(vdata)/ik)))
}

func(c(0.15,4))

optim(par=c(0.1,4),fn=func,method="BFGS")
alpha = 0.1514736 #daily
sigma = 4.0013995 #daily
#alpha = 0.33 #weekly
#sigma = 2.73 #weekly
ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)

vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma)

p3 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("A mixture of Normals",subtitle="alpha=0.15, sigma_2=4.0")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

```{r, include=FALSE}
#vdata = DJ_d$r_Dow_Jones
vdata = DJ_w$r_close
ik = 20
grids = 1:ik/ik

vdata = (vdata - mean(vdata))/sd(vdata)
vq = pnorm(vdata)

p5 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("N(0,1)")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))

df = 3
ndata = vdata*sqrt(df/(df-2))
vq = pt(ndata,df=3)

p6 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("t_3")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))

func <- function(vx){
  alpha = vx[1]
  sigma = vx[2]
  ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)
  
  vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma)
  vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
  vn = c(vn[1],diff(vn))
  return(sum((vn-length(vdata)/ik)**2/(length(vdata)/ik)))
}

func(c(0.15,4))

optim(par=c(0.1,4),fn=func,method="BFGS")
#alpha = 0.1514736 #daily
#sigma = 4.0013995 #daily
alpha = 0.33 #weekly
sigma = 2.73 #weekly
ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)

vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma)

p7 <- ggplot(data.frame(vq), aes(x = vq)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("A mixture of Normals", subtitle="alpha=0.33, sigma_2=2.73")

vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

```{r,include=FALSE}
unifs <- runif(length(vdata),0,1)
p4 <- ggplot(data.frame(unifs), aes(x = unifs)) +
  geom_histogram(breaks = c(0,grids), fill = "lightblue", color = "black") +
  ggtitle("Uniform(0,1)")
```

```{r}
pd <- (p1+p2+p3+p4)
pd + plot_annotation(
  title = 'Dow jones daily log returns')
```

So we have the daily log returns. We know from Rosenblatt that if the null is true, we should see that the bins resemble the bins of the $U(0,1)$ in the bottom right. So essentially, which plot looks the most like it? It is clear that the standard normal null seems unlikely. ALthough the $t_3$ and mixture of normals look more reasonable, but it is still not perfect. But their nulls seem at least more reasonable than the standard normal.

```{r}
pw <- (p5+p6+p7+p4)
pw + plot_annotation(
  title = 'Dow jones weekly log returns')
```

What about weekly log returns? Again, the standard normal null seems unlikely. But now, the $t_3$ null also seems unlikely, and the mixture of normal null looks like the most reasonable. We can look at the computed test statistics in the table below. As we can see, for both daily and weekly log returns, the standard normal had the highest test statistics. For daily log returns, the $t_3$ null had the best test statistic, although we do reject it. For the weekly log returns, we can see as indicated from the plot, that the mixture of normals had the best test statistic, although we still reject the null.

\begin{table}[H]
\begin{tabular}{|c|cc|}
           & \multicolumn{2}{c|}{Test statistic (crit. 1\% = 6.41)}      \\ \hline
$H_0$       & \multicolumn{1}{c|}{daily log returns} & weekly log returns \\ \hline
N(0,1)     & \multicolumn{1}{c|}{3406}              & 431                \\
$t_3$       & \multicolumn{1}{c|}{147}               & 114                \\
Mix of N() & \multicolumn{1}{c|}{212}               & 88                
\end{tabular}
\end{table}

So we can not for either the daily or weekly log returns accept the null of either distributions, but going by the obtained test statistics, for the daily log returns the $t_3$ is the best approximation and for the weekly log returns the mixture of normals is the best approximation.

## B. Dynamical properties of financial return series

```{r setupb, include=F}
data = index_d
```

### 1. Generate log returns. Compute the empirical autocorrelations and partial autocorrelations. Do you find evidence for significant autocorrelations in the individual index return series?

```{r lret}
lret = apply(log(data),2,diff)
summary(lret)
```
Above we generate the log returns, `lret`. Here we also see some summary statistics. From the summary, it is evident that mean and median for all series of the log returns are close to zero. Looking at the minimum and maximum values it seems that there could be skewness in some of the series.

```{r na}
sum(is.na(lret[,'FRCAC40']))
```
We see that we have missing values in the series `FRCAC40`, 393 missing values. Because there is missing values we can not calculate the autocorrelations without first handling these. The simplest way is to omit the missing values, however it is important to note that since there are quite many missing values this could lead to biased estimates or loss of power in the analysis. We proceed by performing listwise deletion. Assuming that we are not supposed to calculate cross-correlations, we proceed as follows:

```{r acfplot, echo=F}
lret_clean <- lret[complete.cases(lret),]
par(mfrow=c(2,4),  cex = 0.5)
for (i in 1:ncol(lret_clean)) {
  serie_i <- colnames(lret_clean)[i]
  Acf(lret_clean[, i], main = paste("ACF for series", serie_i))
}
par(mfrow=c(1,1))
```
Above, you see the ACF-plots for the individual series. We see spikes outside of the confidence interval for all series, indicating that we have significant autocorrelations. Similairly, we can calculate and plot the individual partial autocorrelations:

```{r pacfplot, echo=F}
par(mfrow=c(2,4), cex=0.5)
for (i in 1:ncol(lret_clean)) {
  serie_i <- colnames(lret_clean)[i]
  acf(lret_clean[, i], type = "partial", main = paste("PACF for series", serie_i))
}
par(mfrow=c(1,1))
```
In the plots above we see the individual partial autocorrelations for the individual series. In these plots we see that there are significant spikes for all series, indicating that there is a significant autocorrelation structure in all seven series.

### 2. Compute the Ljung-Box test with respect to 10, 50 and 100 lags. By using the 0.01, 0.05 and 0.1 quantile of the $\chi^2$-distribution with suitable degrees of freedom, interpret your results.

In this question, we are asked to perform the Ljung-Box test. The test statistic is given by:

$$
Q_{LB}(k)=T(T+2)\sum_{j=1}^k \frac{\hat{\rho}_j^2}{T-j} \overset a\sim \chi^2(k-p)
$$
Where $k$ is the number of lags, $T$ is the number of time-points, $\hat{\rho}_j$ is the estimated autocorrelation for the given lag, and $j$ is the specific lag between $1-k$. $p$ is the number of parameters which in this case is 0. This is implemented as an R-function `LB` below. In order to test for different lags for all series, we implemented the function `gridsearch` in order to loop over the different series and lag-values.

```{r LB, include=F}
LB <- function(vx,lag,ip){
  tmp = acf(vx,lag.max=lag,plot=F)$acf
  tmp = tmp[2:(lag+1)]**2
  test = sum(tmp/(length(vx)-1:lag))*length(vx)*(length(vx)+2)
  return(list(test=test, pval=1-pchisq(test,df=lag-ip)))
}

gridsearch <- function(tmp, lag){
  results <- matrix(NA, nrow=length(tmp), ncol = length(lag)*2)
  for (i in 1:length(tmp)){
    tmpi = tmp[i]
    for (j in 1:length(lag)){
      lagj <- lag[j]
      results[i,2*j-1]<-LB(vx=lret[!is.na(lret[,tmpi]),tmpi],lag=lagj,ip=0)$test
      results[i,2*j]<-LB(vx=lret[!is.na(lret[,tmpi]),tmpi],lag=lagj,ip=0)$pval
    }
  }
  return(results)
}
tmp = c(1,2,3,4,5,6,7)
lag = c(10,50,100)
gridsearch(tmp, lag)
```
\begin{table}[h!]
\centering
\begin{tabular}{l c c c c c c}
\hline
           & \multicolumn{2}{c}{Lag=10} & \multicolumn{2}{c}{Lag=50} & \multicolumn{2}{c}{Lag=100} \\
\cline{2-7}
           & Test & p-value & Test & p-value & Test & p-value \\
\hline
DAXINDX    & 15.983  & 0.100  & 68.803  & 0.040  & 118.784 & 0.097 \\
FRCAC40    & 13.224  & 0.211  & 54.640  & 0.303  & 96.416  & 0.583 \\
FTSE100    & 40.943  & 0.000  & 109.873 & 0.000  & 150.905 & 0.001 \\
HNGKNGI    & 64.499  & 0.000  & 133.265 & 0.000  & 179.218 & 0.000 \\
NIKKEI     & 40.195  & 0.000  & 121.768 & 0.000  & 164.491 & 0.000 \\
SNGALLS    & 91.484  & 0.000  & 159.532 & 0.000  & 202.302 & 0.000 \\
SPCOMP     & 30.332  & 0.001  & 119.543 & 0.000  & 192.720 & 0.000 \\
\hline
\end{tabular}
\caption{Results of the LB Test for Different Lags and Indices (rounded to 3 decimal places)}
\label{LB}
\end{table}

In table \ref{LB} we see the results from the Ljung-Box test. We see clearly that FTSE100, HNGKNGI, NIKKEI, SNGALLS, and SPCOMP are significantly auto correlated for $lags = \{10, 50, 100\}$ for 0.01, 0.05 and 0.1 quantile of the $\chi^2$-distribution, as the p-values are below 0.01. 

For DAXINDX we see that $\hat{p}=0.1$ for $lag=10$, indicating no significant autocorrelation at the $95\%$ confidence level (but significant at the $90\%$ level). For $lag=100$, $\hat{p}_{DAXINDX}=0.04$ indicating that we have autocorrelation with $95\%$ significance, but not $99\%$. For $lag = 100$, the p-value of 0.097 suggests that significant autocorrelation is observed at the 90% confidence level.

For FRCAC40, there is no significant autocorrelation for $lags = \{10, 50, 100\}$, as all the p-values are above 0.10, indicating no evidence of autocorrelation at the $90\%$, $95\%$, or $99\%$ significance levels. However, it is important to note that there are missing values in FRCAC40 which likely impacts the reliability of the test-results. As the effective sample size is reduced, it is probable that the test results are inaccurate and could be biased, because of this we are cautios when drawing conclusions about the presence of autocorrelation in FRCAC40.

### 3. Compute the (pairwise) cross-autocorrelations between the individual return series. Do you find evidence for lead-lag relationships?

```{r CCF, echo=F}
# Function to compute cross-autocorrelation matrices for multiple lags
compute_cross_autocorrelation <- function(data, lags) {
  # Get the names of the portfolios (columns)
  series_names <- colnames(data)
  num_portfolios <- length(series_names)
  
  # Initialize a list to store the cross-correlation matrices for each lag
  gamma_matrices <- list()
  
  # Loop through each specified lag
  for (lag in lags) {
    # Initialize an empty matrix to store results for the current lag
    gamma <- matrix(NA, nrow = num_portfolios, ncol = num_portfolios)
    
    # Set row and column names corresponding to series names at different lags
    colnames(gamma) <- paste0(series_names, "$_t$")
    rownames(gamma) <- paste0(series_names, "$_{t-", lag, "}$")
    
    # Loop through each pair of portfolios (time series)
    for (i in 1:num_portfolios) {
      for (j in 1:num_portfolios) {
        # Compute the cross-correlation function for the pair of time series
        ccf_values <- ccf(data[, i], data[, j], plot = FALSE, lag.max = max(lags))
        
        # Find the cross-correlation at the specific lag
        lag_index <- which(ccf_values$lag == lag)
        
        # If the lag exists in the cross-correlation result, store it
        if (length(lag_index) > 0) {
          gamma[i, j] <- ccf_values$acf[lag_index]
        } else {
          gamma[i, j] <- NA  # Store NA if no cross-correlation at this lag
        }
      }
    }
    
    # Store the current gamma matrix for the given lag in the list
    gamma_matrices[[paste0("gamma_", lag)]] <- gamma
  }
  
  return(gamma_matrices)
}

# Define the lags you want to compute (0 to 4)
lags <- 0:4

# Compute the cross-autocorrelation matrices for lags 0 to 4
gamma_matrices <- compute_cross_autocorrelation(lret_clean, lags)

#Print tables
kable(gamma_matrices$gamma_0, digits = 3, format = "latex", escape = FALSE, caption = "$\\hat{\\gamma}_0$")
kable(gamma_matrices$gamma_1, digits = 3, format = "latex", escape = FALSE, caption = "$\\hat{\\gamma}_1$")
kable(gamma_matrices$gamma_2, digits = 3, format = "latex", escape = FALSE, caption = "$\\hat{\\gamma}_2$")
kable(gamma_matrices$gamma_3, digits = 3, format = "latex", escape = FALSE, caption = "$\\hat{\\gamma}_3$")
kable(gamma_matrices$gamma_4, digits = 3, format = "latex", escape = FALSE, caption = "$\\hat{\\gamma}_4$")

```
```{r gamma2, echo=F}
gamma_matrices$gamma_1-t(gamma_matrices$gamma_1)
gamma_matrices$gamma_2-t(gamma_matrices$gamma_2)
gamma_matrices$gamma_3-t(gamma_matrices$gamma_3)
gamma_matrices$gamma_4-t(gamma_matrices$gamma_4)
```

### 4. Generate squared log returns. Analyze their dynamical properties. What do you find?
By squaring the log returns, we achieve second moments of the time series which are relevant for discussion regarding volatility and understanding the behaviour of volatility. We will mainly examine three properties, namely volatility dynamics, stationarity of variance, and persistence in volatility. 


We first start by examining the ACF and PACF. 

```{r squared returns}
sqlret <- apply(lret_clean, 2, function(x) x^2 )
```

```{r create all the ACF and PACF necessary, fig.show = "hide"}
acf_plt <- pacf_plt <- list()
for (i in 1:ncol(sqlret)) {
   acf_obj <- acf(sqlret[!is.na(sqlret[,i]),i], main = paste("ACF for series", colnames(sqlret)[i]), plot = FALSE)
  acf_plt[[i]] <- acf_obj
}
for (i in 1:ncol(sqlret)) {
   pacf_obj <- pacf(sqlret[!is.na(sqlret[,i]),i], main = paste("PACF for series", colnames(sqlret)[i]), plot = FALSE)
  pacf_plt[[i]] <- pacf_obj
}

names(acf_plt) <- names(pacf_plt) <- colnames(sqlret)
for (i in 1:length(names(acf_plt))) {
  plot(acf_plt[[i]], main = paste("ACF for series", names(acf_plt)[i]))
}


names(pacf_plt) <- colnames(sqlret)
for (i in 1:length(names(pacf_plt))) {
  plot(pacf_plt[[i]], main = paste("PACF for series", names(pacf_plt)[i]))
}
```


```{r plots DAXINDX}
par(mfrow = c(1,3))

# Plot ACF for the first series
tmp <- 1
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```
For DAXINDX we can observe that the ACF plot is slowly decaying with significant autocorrelations over multiple lags. This would indicate volatility clustering and time-varying variance. This finding is according to theory where a period of high volatility is followed by high volatility and a period of low volatility is followed by low volatility. This finding is also evident in the plot of squared returns. The PACF has quite sharp cut indicating a short-term dependency in volatility while the ACF plot indicates a long-term dependency. The fact that we observe both a short-term and long-term dependency indicates that the variance is not constant over time but rather depends on past values which we interpret as that the time series exhibits conditional heteroskedasticity. 

```{r plots for FRCAC40}
par(mfrow = c(1, 3))
tmp <- 2
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```
These findings are very similar to the findings observed when analyzing the German index (DAXINDX). We observe the same slowly decaying ACF, the PACF observe quite sharp cut off after the first few lags which leads us to interpret it that this time series also exhibit both short-term and long-term dependency, once again indicating on conditional heteroskedasticity. Further on, all plots indicate on clustered volatility. 


```{r plots for FTSE100}
par(mfrow = c(1, 3))

# Plot ACF for the first series
tmp <- 3
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```
For the UK-based Financial Time Stock Exchange we find that the main difference is in the PACF-plot which is cut off within the first couple of lags. The second lag already is quite small in relation to the first. This would indicate a weaker short-term memory but since the ACF is slowly decaying and we can observe some clusters in the right-most plot, the time series still exhibits some properties that it is reactive to recent shocks. The slowly decaying property of the ACF-plot can be some evidence towards the mean-reverting property in addition to the theory regarding volatility clustering. 


```{r plots for HNGKNGI}
par(mfrow = c(1, 3))

# Plot ACF for the first series
tmp <- 4
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```

For the Hong-kong based index we observe a sharp cut off in the ACF plot. We still observe three significant lags which would be lag 1, 3, and 5. The PACF confirms this finding which indicates that there is some short-term dependency. Since the volatility is influenced by previous recent lags this would still indicate some clustering and responsiveness to recent shocks but no long-term dependency. This could indicate on a stationary variance, however, the plotted squared returns show some spikes and we still have the fact the time series shows short-term dependency. All in all, the clustering phenomena seems quite week which can also be observed if we compare the plotted square logarithmic returns with for example the graph of FRCAC40. 

```{r plots for NIKKEI}
par(mfrow = c(1, 3))

# Plot ACF for the first series
tmp <- 5
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```
For the Japanese index we observe a quite sharp cut-off in the ACF plot but the values are all significant. Even further, we observe some significant spikes between lags 15 and 20 and around 28. This indicates that the volatility is predominantly influenced by short-term memory. The observations around lag 15-20 and 28 could indicate that there is some seasonality or other external factor which makes the underlying patterns in volatility appear at regular time intervals. For the plot of squared returns we observe some high spikes, followed by decaying spikes. This could indicate on some conditional heteroskedasticity. 


```{r plots for SNGALLS}
par(mfrow = c(1, 3))

# Plot ACF for the first series
tmp <- 6
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```

In the ACF plot we observe that the first four lags are significant and relatively stable, interrupted by lag number three which is also significant but much smaller. After lag 5 we observe quite many significant lags but that are barely significant. In the PACF plot we observe that lag 2 is not significant but 1, 3, and 4 is significant. Also 8, 9 and 12 are significant. For this reason it seems that the short-term dependency is stronger than the long-term according to the ACF-plot. The significance of lags 8, 9, and 12 could indicate on some periodic effect. In the plotted squared returns we can observe some very meaningful clustering. 


```{r plots for SPCOMP}
par(mfrow = c(1, 3))

# Plot ACF for the first series
tmp <- 7
plot(acf_plt[[tmp]], main = paste("ACF for series", names(acf_plt)[tmp]))
plot(pacf_plt[[tmp]], main = paste("PACF for series", names(pacf_plt)[tmp]))
plot(sqlret[, tmp], type = "l", main = "Plot of squared returns", ylab = colnames(sqlret)[tmp])
```

This time series shows some short-term dependencies but nothing significant in the long run. Both the ACF and PACF plots indicates on this observations. Up to around lag 5 is significant for both with lag 8 being barely significant in the ACF plot. This means that the volatility is mainly being driven by recent shocks. Also the plot of squared returns show weaker signs of clustering but of course, there are some clustering which can be confirmed by the ACF and PACF plots since almost all lags up to 5 are significant in both plots. 

```{r corr(y_t^2, y_t-1)}
# Calculate lagged returns (y_{t-1})
i <- 2
correlation <- numeric(ncol(sqlret))
names(correlation) <- colnames(sqlret)
for (i in 1:ncol(sqlret)) {
  returns <- lret_clean[, i]
  
  lagged_returns <- c(NA, head(returns, -1))
  tail(returns, 20); tail(lagged_returns, 20)
  length(sqlret[, 1]);length(lagged_returns)
  
  
  valid_indices <- complete.cases(sqlret[, i], lagged_returns)

  # Filter both series to include only valid rows
  filtered_returns <- sqlret[valid_indices, i]
  filtered_lagged_returns <- lagged_returns[valid_indices]

  # Calculate correlation
  correlation[i] <- cor(filtered_returns, filtered_lagged_returns)
}
print(correlation)
```

the negative correlations between squared logarithmic returns and lagged logarithmic returns show a weak inverse relationship between current volatility and past returns. This can bet interpreted as after a large market movement, volatility tends to decrease in the next period. However, in practice we have most often seen that a large market movement often indicates a higher volatility in the subsequent periods. This inverse relationship can also be according to the theory that a large negative movement will increase volatility more than a positive shock of the same magnitude (leverage effect). 

Further on, the low magnitude shows that past returns have a weak relationship with future volatility. Lastly, these values indicates also that there might be some weak clustering although when we observe in more detail each lag we have seen much stronger evidence of volatility clustering. 





