---
title: "Report for PS 1"
author:
- Mark Becker, Erik Lillrank & Vilim Nedic
- Another Author
date: ""
output:
  pdf_document:
    keep_tex: yes
  df_print: kable
  html_document:
    df_print: paged
toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(FE)
library(psych)
library(forecast)
data("DJ_w")
data("DJ_d")
```

#  Statistical Properties of Asset Returns

## A. Distributional properties of Dow Jones index returns

### 1. Plot the series of log returns and compute descriptive statistics. Which distributional properties do you find? Do you find differences between daily and weekly data?

```{r PLOT1, echo=F}
ggplot(DJ_d, aes(y=r_Dow_Jones,x=1:nrow(DJ_d))) + geom_line() +
  labs(x='time horizon',y='daily log return')

ggplot(DJ_w, aes(y=r_close,x=1:nrow(DJ_w))) + geom_line() +
  labs(x='time horizon',y='weekly log return')

describe(DJ_d$r_Dow_Jones)
describe(DJ_w$r_close)

```
Both series have means of zero. Daily returns has standard deviation 0.01 and weekly returns 0.03. Daily returns has a skewness of -0.96 and kurtosis of 36.6.Weekly returns has a skewness of  -1.09 and kurtosis is 15.06. Since a standard normal distribution has a standard deviation of 1, skewness of 0 and kurtosis of 3, this is strong evidence that the returns are not normally distributed. We can further investigate which distribution seems most likely by producing QQ plots.


### 2. Evaluate the empirical distributions of the index log returns using quantile-quantile plots (QQ-plots). Test the empirical distribution against

#### a) a normal distribution (with same mean and variance)

```{r ndist}
ndistd <- ggplot(DJ_d, aes(sample=r_Dow_Jones))
ndistd + geom_qq(distribution=stats::qnorm, dparams = list(mean = mean(DJ_d$r_Dow_Jones), sd = sd(DJ_d$r_Dow_Jones))) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red") +
  labs(y = "Daily returns")
qqnorm(DJ_d$r_Dow_Jones)

ndistw <- ggplot(DJ_w, aes(sample=r_close))
ndistw + geom_qq(distribution=stats::qnorm, dparams = list(mean = mean(DJ_w$r_close), sd = sd(DJ_w$r_close))) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red") +
  labs(y = "Weekly returns")
qqnorm(DJ_w$r_close)
```
The first thing we observe when studying above plots are that the line intersects the data in the middle quite good. When it comes to the tails (lower left corner and upper right corner), we observe that the normal distribution captures these observations badly. This would indicate on a heavier-tailed data than the normal distribution would show. This finding is in accordance with other findings when studying econometric data as these are often more heavy-tailed.

#### b) alternative tn-distributions with n degrees of freedom.

 
```{r tdist}
df = 3
tdistd = ggplot(DJ_d, aes(sample=r_Dow_Jones/sd(r_Dow_Jones)*sqrt(df/(df-2))))
tdistd + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red") + 
  labs(y = "Daily returns")
qqplot(rt(length(DJ_d$r_Dow_Jones),df=3),DJ_d$r_Dow_Jones)

tdistw = ggplot(DJ_w, aes(sample=r_close/sd(r_close)*sqrt(df/(df-2))))
tdistw + geom_qq(distribution=stats::qt, dparams=list(df=df)) + geom_abline(aes(intercept=0,slope=1),color="red") + 
  labs(y = "Weekly returns")
qqplot(rt(length(DJ_w$r_close),df=3),DJ_w$r_close)
```


A visual analysis of the QQ-plots show that both empirical distribution could follow a t-distribution with 3 degrees of freedom. Once again we observe that the t-distribution have a hard time capturing these heavier tails but it captures it much better than the normal could. We still observe some limited discrepancy in the lower left corner where weekly returns show one outlier while daily returns show two outliers. In the upper right corner we observe that the discrepancy between the theoretical distribution and the underlying distribution is bigger. There is especially one outlier that is poorly captured. This outlier and some other extreme observations are closer to the theoretical distribution when studying the daily returns compared to the weekly returns as the distance to the theoretical distribution is smaller in the daily returns. 



### 3. Compute a $\chi^2$-goodness-of-fit test against...

The $\chi^2$ test statistic is

$$
\chi^2=\sum_{j=1}^k \frac{(N_j-Np_j)^2}{Np_j} \sim^a \chi^2_{k-s-1},
$$

where $k$ is the number of categories, $N_j$ is the number of observations in the $j$th category, $p_j$ is the estimated probability for an observation in category $j$ and $s$ is the number of parameters to be estimated.

```{r}
r_it = DJ_d$r_Dow_Jones
r_it = DJ_w$r_close
r_it_stand = (r_it - mean(r_it))/sd(r_it)

k = 20
category = 1:k/k
```

#### a) a normal distribution

$H_0:$ The true distribution is the standard normal distribution.
```{r}
r = DJ_w$r_close
rstd = (r - mean(r))/sd(r)
k = 20
categories = 1:k/k

N <- length(r)
pj = 1/k

cum_probs = pnorm(rstd)
cum_N = NULL; for(val in categories) cum_N = c(cum_N,sum(cum_probs <= val))
Nj = c(cum_N[1],diff(cum_N))

test = sum((Nj-N*pj)^2/(N*pj))
cat("test =",test," df =",k-3," p-value =",1-pchisq(test,df=k-3))

vdata = DJ_d$r_Dow_Jones
vdata = DJ_w$r_close
vdata = (vdata - mean(vdata))/sd(vdata)
ik = 20
grids = 1:ik/ik

```
We reject that the data follows $N(0,1)$ on a 1% significance level.

#### b) a $t_n$-distribution with n degrees of freedom
```{r}
df = 3
ndata = vdata*sqrt(df/(df-2))

vq = pt(ndata,df=5); hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))
```

#### c) a mixture of normal distributions with mixture probability $\alpha$ and variance $\sigma_2^2$
```{r}
alpha = 0.1514736
sigma = 4.0013995
alpha = 0.33
sigma = 2.73
ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)

vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma); hist(vq)
vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
vn = c(vn[1],diff(vn))
test = sum((vn-length(vdata)/ik)**2/(length(vdata)/ik))
cat("test =",test," df =",ik-3," p-value =",1-pchisq(test,df=ik-3))

func <- function(vx){
  alpha = vx[1]
  sigma = vx[2]
  ndata = vdata*sqrt((1-alpha) + alpha*sigma**2)
  
  vq = (1-alpha)*pnorm(ndata) + alpha*pnorm(ndata,sd=sigma)
  vn = NULL; for(val in grids) vn = c(vn,sum(vq <= val))
  vn = c(vn[1],diff(vn))
  return(sum((vn-length(vdata)/ik)**2/(length(vdata)/ik)))
}

func(c(0.15,4))

optim(par=c(0.1,4),fn=func,method="BFGS")
```


## B. Dynamical properties of financial return series

```{r setupb, include=F}
data = index_d
```

### 1. Generate log returns. Compute the empirical autocorrelations and partial autocorrelations. Do you find evidence for significant autocorrelations in the individual index return series?

```{r lret}
lret = apply(log(data),2,diff)
summary(lret)
```
Above we generate the log returns, `lret`. Here we also see some summary statistics. From the summary, it is evident that mean and median for all series of the log returns are close to zero. Looking at the minimum and maximum values it seems that there could be skewness in some of the series. Below the log returns are plotted:

```{r plotlret}
matplot(lret,type='l',ylab='log returns',xlab='time horizon')
```
Here the plot shows low volatility with smaller spikes for most time periods. For the time period slightly below 500 we see a larger spike indicating stronger volatility during this period, possibly due to some extreme market event. The log returns are centered around zero over time, suggesting that there is no significant trend.

```{r na}
sum(is.na(lret[,'FRCAC40']))
```
We see that we have missing values in the series `FRCAC40`, 393 missing values. Because there is missing values we can not calculate the autocorrelations without first handling these. The simplest way is to omit the missing values, however it is important to note that since there are quite many missing values this could lead to biased estimates or loss of power in the analysis. We proceed by performing listwise deletion. Assuming that we are not supposed to calculate cross-correlations, we proceed as follows:

```{r acfplot}
lret_clean <- lret[complete.cases(lret),]
par(mfrow=c(2,4),  cex = 0.5)
for (i in 1:ncol(lret_clean)) {
  serie_i <- colnames(lret_clean)[i]
  Acf(lret_clean[, i], main = paste("ACF for series", serie_i))
}
par(mfrow=c(1,1))
```
Above, you see the ACF-plots for the individual series. We see spikes outside of the confidence interval for all series, indicating that we have significant autocorrelations. Similairly, we can calculate and plot the individual partial autocorrelations:

```{r pacfplot}
par(mfrow=c(2,4), cex=0.5)
for (i in 1:ncol(lret_clean)) {
  serie_i <- colnames(lret_clean)[i]
  acf(lret_clean[, i], type = "partial", main = paste("PACF for series", serie_i))
}
par(mfrow=c(1,1))
```
In the plots above we see the individual partial autocorrelations for the individual series. In these plots we see that there are significant spikes for all series, indicating that there is a significant autocorrelation structure in all seven series.

### 2. Compute the Ljung-Box test with respect to 10, 50 and 100 lags. By using the 0.01, 0.05 and 0.1 quantile of the $\chi^2$-distribution with suitable degrees of freedom, interpret your results.

In this question, we are asked to perform the Ljung-Box test. It is not entirely clear from the question whether this refers to individual tests for each series, or a multivariate Ljung-Box test. Starting with the univariate case. The test statistic is given by:

$$
Q_{LB}(k)=T(T+2)\sum_{j=1}^k \frac{\hat{\rho}_j^2}{T-j} \overset a\sim \chi^2(k-p)
$$
Where $k$ is the number of lags, $T$ is the number of time-points, $\hat{\rho}_j$ is the estimated autocorrelation for the given lag, and $j$ is the specific lag between $1-k$. $p$ is the number of parameters which in this case is 0. This is implemented as an R-function `LB` below. In order to test for different lags for all series, we implemented the function `gridsearch` in order to loop over the different series and lag-values.

```{r LB, message=F}
LB <- function(vx,lag,ip){
  tmp = acf(vx,lag.max=lag,plot=F)$acf
  tmp = tmp[2:(lag+1)]**2
  test = sum(tmp/(length(vx)-1:lag))*length(vx)*(length(vx)+2)
  return(list(test=test, pval=1-pchisq(test,df=lag-ip)))
}

gridsearch <- function(tmp, lag){
  results <- matrix(NA, nrow=length(tmp), ncol = length(lag)*2)
  for (i in 1:length(tmp)){
    tmpi = tmp[i]
    for (j in 1:length(lag)){
      lagj <- lag[j]
      results[i,2*j-1]<-LB(vx=lret[!is.na(lret[,tmpi]),tmpi],lag=lagj,ip=0)$test
      results[i,2*j]<-LB(vx=lret[!is.na(lret[,tmpi]),tmpi],lag=lagj,ip=0)$pval
    }
  }
  return(results)
}
tmp = c(1,2,3,4,5,6,7)
lag = c(10,50,100)
gridsearch(tmp, lag)
```
\begin{table}[h!]
\centering
\begin{tabular}{l c c c c c c}
\hline
           & \multicolumn{2}{c}{Lag=10} & \multicolumn{2}{c}{Lag=50} & \multicolumn{2}{c}{Lag=100} \\
\cline{2-7}
           & Test & p-value & Test & p-value & Test & p-value \\
\hline
DAXINDX    & 15.983  & 0.100  & 68.803  & 0.040  & 118.784 & 0.097 \\
FRCAC40    & 13.224  & 0.211  & 54.640  & 0.303  & 96.416  & 0.583 \\
FTSE100    & 40.943  & 0.000  & 109.873 & 0.000  & 150.905 & 0.001 \\
HNGKNGI    & 64.499  & 0.000  & 133.265 & 0.000  & 179.218 & 0.000 \\
NIKKEI     & 40.195  & 0.000  & 121.768 & 0.000  & 164.491 & 0.000 \\
SNGALLS    & 91.484  & 0.000  & 159.532 & 0.000  & 202.302 & 0.000 \\
SPCOMP     & 30.332  & 0.001  & 119.543 & 0.000  & 192.720 & 0.000 \\
\hline
\end{tabular}
\caption{Results of the LB Test for Different Lags and Indices (rounded to 3 decimal places)}
\end{table}


### 3. Compute the (pairwise) cross-autocorrelations between the individual return series. Do you find evidence for lead-lag relationships?

```{r CCF}
# Set up plotting grid
par(mfrow = c(2, 4), cex=0.5)  # Adjust grid size as needed

# Loop through all unique pairs of columns
for (i in 1:(ncol(lret_clean) - 1)) {
  for (j in (i + 1):ncol(lret_clean)) {
    # Get the column names for the pair
    series_i <- colnames(lret_clean)[i]
    series_j <- colnames(lret_clean)[j]
    
    # Compute and plot the cross-correlation function
    ccf(lret_clean[, i], lret_clean[, j], plot = TRUE, 
        main = paste("CCF for", series_i, "vs", series_j))
  }
}

# Reset plotting layout
par(mfrow = c(1, 1))

```


### 4. Generate squared log returns. Analyze their dynamical properties. What do you find?
